\section*{8.2 Power Iterations}%{{{1

\subsection*{8.2.1 The Power method} %{{{2

This is a method that can be used on a general square, non singular and real matrix 
$A\in\mathbb R^{(n\times n)}$.
Given a 2-norm $q^{(0)}\in\mathbb R^n$, this method produces a sequence of vectors 
$q^{(k)}$
%
%
\begin{algo}
{
%
	(The Power Method):
%
}\\
\textbf{Input: }
{
%
	\\$q^{(0)}\in\mathbb R^n$.
	\\$A\in\mathbb R^{n\times n}$.
%
}\\
\textbf{Output: }
{
%
	\\One eigenvector $q^{(k)}$.
%
}\\
\line(1,0){150}
\begin{algorithmic}
%
\For{$k=1,2,3,\dots,k$}
	\State{$z\gets A q^{(k-1)}$}
	\State{$q^{(k)}\gets z/\norm{z}_2$}
	\State{$\lambda^{(k)} \gets [q^{(k)}]^T A q^{(k)}$}
\EndFor{}
%
\end{algorithmic}
\line(1,0){150}
\end{algo}
%
%
As long as the eigenvector of the eigenvalue with the largest eigenvalue is unique,
$q^{(k)}$ will converge to an eigenvector.
%
The convergence is proportional to $|\lambda_1/\lambda_2|^{2k}$ where $\lambda_1$ is the
eigenvalue with the largest modulo, and $\lambda_2$ is the eigenvalue with the second largest modulo.
%
The error $|\lambda - \lambda_k|$ for some $\lambda\in\lambda(A)$ can be shown to be
\begin{equation}
	|\lambda - \lambda_k| \le \norm{Aq^{(k)} - \lambda^{(k)}q^{(k)}}_2\sqrt 2.
\end{equation}

%}}}2

\subsection*{8.2.2 Inverse Iteration} %{{{2

Suppose that the power method is applied with $A$ replaced with $(A-\lambda I)^{-1}$
given that $\lambda$ is very close to, but not equal to, one of the eigenvalues of $A$.
This matrix is nearly singular, and can easily be proven to have the same eigenvectors as $A$.
Assume that $A$ has the eigenvalues $\lambda^i$ with the corresponding eigevectors $v_i$, and that 
$xv_i^T = a_i$. Now
\begin{align}
	(A-\lambda I)^{-1}x = \sum_i a_i/(\lambda-\lambda_i) v_i.
\end{align}
If $\lambda\approx\lambda_i$ than the component of $x$ that is parallel to $v_i$
will become dominant.

%}}}2

\subsection*{8.2.3 The Rayleigh Quotient Iteration} %{{{2
This algorithm is an extended version of the inverse iteration algorithm where
the value of $\lambda$ is updated each iteration.
\begin{equation}
	\lambda = r(x) = \frac{x^TAx}{x^Tx}
\end{equation}
where $r(x)$ is called the Rayleigh quotient of $x$.
%
%
\begin{algo}
{
%
	(The Raileigh Quotient Method):
%
}\\
\textbf{Input: }
{
%
	\\$x_0\in\mathbb R^n$, $\norm{x_0}_2=0$.
	\\$A\in\mathbb R^{n\times n}$.
%
}\\
\textbf{Output: }
{
%
	\\An eigenvector $x_k$ and its corresponding eigenvalue $\lambda$.
%
}\\
\line(1,0){150}
\begin{algorithmic}
%
\For{$k=1,2,3,\dots,k$}
	\State{$\mu_k\gets r(x_k)$}
	\State{solve $(A-\mu k I)z_{k+1} = x_k$ for $z_{k+1}$}
	\State{$x_{k+1} \gets z_{k+1}/\norm{z_{k+1}}_2$}
\EndFor{}
%
\end{algorithmic}
\line(1,0){150}
\end{algo}
%
%
The Raileigh Quotient Method almost always converges, and when it does the convergence is cubic.

%}}}2

\subsection*{8.2.3 Orthogonal Iteration} %{{{

The power method can be generalized to compute higher dimensional invariant subspaces.
Let $r$ be an integer $r\in[1,n]$, and let $Q$ be an matrix $Q\in\mathbb R^{n\times r}$.
The method of orthogonal iteration generates a sequence of matrices $\{ Q_k \}$
%
%
\begin{algo}
{
%
	(Orthogonal iteration):
%
}\\
\textbf{Input: }
{
%
	\\$Q_0=I,\,Q_k\in\mathbb R^{n\times n}$.
	\\$A\in\mathbb R^{n\times n}$.
%
}\\
\textbf{Output: }
{
%
	\\$\ran{Q_k(:,1:r)}=D_r(A)$ ($D=$ the dominant subspace of $A$ of dimension $r$).
%
}\\
\line(1,0){150}
\begin{algorithmic}
%
\For{$k=1,2,3,\dots,k$}
	\State{$Z_k\gets A Q_{k-1}$}
	\State{$Q_kR_k\gets Z_k$ (QR factorization)}
\EndFor{}
%
\end{algorithmic}
\line(1,0){150}
\label{algOrthoIi}
\end{algo}
%
%
The vector $Qe_1$ will converge in exactly the same way as $q^{(k)}$ in the power method.
$Qe_2$ can be any vector in $\ran{A}$ that is orthogonal to $Qe_1$, so it will normally converge to
the eigenvector with the second largest eigenvalue. 
$Qe_3$ will converge to the eigenvector with the third largest eigenvalue and so on.

The convergence of the method is proportional to $|\lambda_r/\lambda_{r+1}|$.
%The following theorem is useful to understand the method.
%\begin{theorem}
%(Invariant subspaces):
%Assume that $A\in\mathbb R^{(n\times n)}$ is symmetric and that 
%\begin{equation}
%Q=
%\begin{matrix}
%	\\
%	\smatrix{Q_1 & Q_2} 
%	\\
%	\begin{matrix}
%		r & n-r
%	\end{matrix}
%\end{matrix}
%\end{equation}
%is orthogonal. If $\ran(Q_1)$ is an invariant subspace, then
%\begin{equation}
%Q^TAQ = D =
%\begin{matrix}
%	\\
%	\smatrix{D_1 & 0 \\ 0 & D_2} 
%	&
%	\begin{matrix}
%		r \\ n-r
%	\end{matrix}
%	\\
%	\begin{matrix}
%		r & n-r
%	\end{matrix}
%\end{matrix}
%\end{equation}
%and $\lambda(A) = \lambda(D_1)\bigcup\lambda(D_2)$.
%\end{theorem}

%}}}2

\subsection*{8.2.4 The QR Iteration}%{{{2

Assume that $|\lambda_1|>|\lambda_2|>\dots>\lambda_n$ and that 
$Q^TTQ=\diag(\lambda_1,\lambda_2,\dots,\lambda_3)$ is the Shur decomposition of $A$.
(This is always true if $A$ is symmetric and none of the eigenvalues are degenerate.)
When the method of orthogonal iteration with $r=n$ is applied to $A$, the columns of $Q$ 
will convergence to the eigenvectors of $A$ and $R$ will converge to 
$\diag(\lambda_1,\lambda_2,\dots,\lambda_3)$.

Assume that 
\begin{equation}
	T_k = Q_k^T A Q_k
\end{equation}
The QR iteration arises when we calculate $T_n$ directly from $T_{n-1}$. 
%
We use the formula from  Alg. \ref{algOrthoIi} (orthogonal iteration).
\begin{equation}
	Q_kR_k = AQ_{k-1},\quad \Rightarrow \quad A = 	Q_kR_kQ_k^T \label{eq824_1} 
\end{equation}
Then
\begin{align}
	&T_{k-1} = Q_{k-1} A Q_{k-1}  = Q_{k-1}^T Q_{k} R_{k} = \tilde Q_k R_k\label{eq824_2} \\
	&T_k = Q_k A Q_{k}  = R_k Q_{k-1}^T Q_{k} = R_k \tilde Q_k 	\label{eq824_3} 
\end{align}
Note that Eq. \eqref{eq824_2} is the QR factorization of $T_{k-1}$ since the product of two
square orthogonal matrices is a new square orthonormal matrix. 
The sequence of $T_k$ can be found
%
%
\begin{algo}
{
%
	(QR iteration):
%
}\\
\textbf{Input: }
{
%
	\\$Q_0=I,\,Q_0\in\mathbb R^{n\times n}$.
	\\$A\in\mathbb R^{n\times n}$.
%
}\\
\textbf{Output: }
{
%
	\\$T_k$ is the Shur decomposition of $A$. 
	\\$Q_ke_i$ is the $i$'th eigenvector of $A$.
%
}\\
\line(1,0){150}
\begin{algorithmic}
%
\State{$T_0\gets \tilde Q_0^T A \tilde Q_{0}$}
\For{$k=1,2,3,\dots,k$}
	\State{$\tilde Q_kR_k\gets T_{k-1}$ (QR factorization)}
	\State{$T_k\gets R_{k}\tilde Q_{k-1}$}
\EndFor{}
%
\end{algorithmic}
\line(1,0){150}
\label{algQRIterSimple}
\end{algo}
%
%
Since $T_k = R_k\tilde Q_k =\tilde Q_k(\tilde Q_k R_k)\tilde Q^k = \tilde Q^T_kT_{k-1}\tilde Q_k$
it follows from induction that 
$T_k = (\tilde Q_0\tilde Q_1\dots \tilde Q^k)^T A (\tilde Q_0\tilde Q_1\dots \tilde Q^k)$ is
similar to $A$.
Also note that the QR-iteration simply is a reformulation of
the orthogonal iteration algorithm, since Eq. \eqref{eq824_2} and \eqref{eq824_3} is based on
Eq. \eqref{eq824_1}.

Each QR-iteration requires $\mathcal O(n^3)$ flops and the algorithm only converge linearly. 
It is, in other words, not a very efficient way of computing the Shur decomposition.
%
The convergence of $\text{dist}( \text{span}(q_1^{(1)},\dots,q_i^{(1)}),\text{span}(q_1^{(k)},\dots,q_i^{(k)}) \propto|\lambda_i/\lambda_{i+1}|^k$

%}}}1
%vim:foldmethod=marker
